{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhYTtD6notmHJpYb678fsc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ensama-cmd/CivilEngineeringAI/blob/main/Notebooks/04_fine_tuning_NPL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEFLOU0yRHUv"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning d'un modèle pour l'extraction des paramètres de construction\n",
        "\n",
        "# Installation des dépendances\n",
        "!pip install transformers datasets evaluate seqeval accelerate\n",
        "\n",
        "# Import des bibliothèques\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Chargement du tokenizer\n",
        "model_checkpoint = \"dslim/bert-base-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Exemple de données annotées (à compléter avec de vraies données)\n",
        "# Format: [(\"texte\", {\"entities\": [(start, end, label), ...]}), ...]\n",
        "training_data = [\n",
        "    (\n",
        "        \"Maison individuelle de 120m² avec 2 étages\",\n",
        "        {\"entities\": [(0, 5, \"TYPE\"), (24, 29, \"SURFACE\"), (35, 36, \"ETAGES\")]}\n",
        "    ),\n",
        "    (\n",
        "        \"Immeuble de 5 étages en béton de 800m²\",\n",
        "        {\"entities\": [(0, 7, \"TYPE\"), (11, 12, \"ETAGES\"), (19, 24, \"MATERIAU\"), (28, 32, \"SURFACE\")]}\n",
        "    ),\n",
        "    # Ajouter plus d'exemples ici...\n",
        "]\n",
        "\n",
        "# Préparation des données pour le fine-tuning\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Conversion des données au format requis\n",
        "def convert_to_hf_format(data):\n",
        "    tokens = []\n",
        "    ner_tags = []\n",
        "\n",
        "    for text, annotation in data:\n",
        "        words = text.split()\n",
        "        tags = [\"O\"] * len(words)\n",
        "\n",
        "        for start, end, label in annotation[\"entities\"]:\n",
        "            # Trouver les mots couverts par l'entité\n",
        "            entity_text = text[start:end]\n",
        "            entity_words = entity_text.split()\n",
        "\n",
        "            # Marquer le premier mot avec B- et les suivants avec I-\n",
        "            for i, word in enumerate(words):\n",
        "                if word == entity_words[0]:\n",
        "                    tags[i] = f\"B-{label}\"\n",
        "                    for j in range(1, len(entity_words)):\n",
        "                        if i+j < len(words) and words[i+j] == entity_words[j]:\n",
        "                            tags[i+j] = f\"I-{label}\"\n",
        "\n",
        "        tokens.append(words)\n",
        "        ner_tags.append([tag_to_id(tag) for tag in tags])\n",
        "\n",
        "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
        "\n",
        "# Mapping des tags vers IDs\n",
        "def tag_to_id(tag):\n",
        "    tag_map = {\n",
        "        \"O\": 0,\n",
        "        \"B-TYPE\": 1, \"I-TYPE\": 2,\n",
        "        \"B-SURFACE\": 3, \"I-SURFACE\": 4,\n",
        "        \"B-ETAGES\": 5, \"I-ETAGES\": 6,\n",
        "        \"B-MATERIAU\": 7, \"I-MATERIAU\": 8\n",
        "    }\n",
        "    return tag_map.get(tag, 0)\n",
        "\n",
        "# Conversion des données\n",
        "hf_data = convert_to_hf_format(training_data)\n",
        "dataset = Dataset.from_dict(hf_data)\n",
        "\n",
        "# Tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Chargement du modèle\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=9,  # Nombre de labels\n",
        "    id2label={0: \"O\", 1: \"B-TYPE\", 2: \"I-TYPE\", 3: \"B-SURFACE\", 4: \"I-SURFACE\",\n",
        "              5: \"B-ETAGES\", 6: \"I-ETAGES\", 7: \"B-MATERIAU\", 8: \"I-MATERIAU\"},\n",
        "    label2id={\"O\": 0, \"B-TYPE\": 1, \"I-TYPE\": 2, \"B-SURFACE\": 3, \"I-SURFACE\": 4,\n",
        "              \"B-ETAGES\": 5, \"I-ETAGES\": 6, \"B-MATERIAU\": 7, \"I-MATERIAU\": 8}\n",
        ")\n",
        "\n",
        "# Arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"civil-engineering-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Métrique d'évaluation\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# Entraînement\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Lancement de l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "# Sauvegarde du modèle\n",
        "model.save_pretrained(\"civil-engineering-ner-model\")\n",
        "tokenizer.save_pretrained(\"civil-engineering-ner-model\")"
      ]
    }
  ]
}